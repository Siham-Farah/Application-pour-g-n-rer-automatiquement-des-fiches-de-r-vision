{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe8f94-91c7-4ccf-a02a-a284fe67f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les stopwords français\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "# Charger le modèle SpaCy pour le français\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# Connexion à MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Coursdb\"]  # Nom de ta base de données\n",
    "\n",
    "# Liste des collections à traiter (ex: modules de cours)\n",
    "modules = db.list_collection_names()\n",
    "modules\n",
    "def nettoyer_texte(texte):\n",
    "    \"\"\" Nettoie le texte : suppression de la ponctuation, mise en minuscules et suppression des stopwords. \"\"\"\n",
    "    texte = texte.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(texte, language=\"french\")\n",
    "    tokens = [mot for mot in tokens if mot not in stop_words and len(mot) > 2]  # Suppression des mots vides et courts\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def extraire_mots_cles_tfidf(documents):\n",
    "    \"\"\" Applique TF-IDF pour extraire les mots-clés les plus pertinents dans un corpus de documents. \"\"\"\n",
    "    corpus = [doc[\"content\"] for doc in documents]\n",
    "    \n",
    "    # Vérification des documents non vides\n",
    "    corpus_non_vide = [texte for texte in corpus if len(texte.strip()) > 0]  # Filtrer les documents vides\n",
    "\n",
    "    if not corpus_non_vide:\n",
    "        return []  # Retourner une liste vide si tous les documents sont vides\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=10)  # Extraire les 10 mots les plus importants\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus_non_vide)\n",
    "    mots_cles = vectorizer.get_feature_names_out()\n",
    "    return mots_cles\n",
    "\n",
    "\n",
    "def extraire_concepts(texte, taille_max=1_000_000):\n",
    "    \"\"\" Divise un texte en morceaux et applique SpaCy sur chaque partie. \"\"\"\n",
    "    nlp.max_length = 10_000_000  # Augmente la limite si nécessaire\n",
    "    concepts = set()\n",
    "    \n",
    "    # Découper le texte en morceaux\n",
    "    for i in range(0, len(texte), taille_max):\n",
    "        morceau = texte[i:i + taille_max]\n",
    "        doc = nlp(morceau)\n",
    "        for ent in doc.ents:\n",
    "            concepts.add(ent.text)\n",
    "\n",
    "    return list(concepts)\n",
    "\n",
    "\n",
    "def creer_graphe_connaissance(concepts):\n",
    "    \"\"\" Crée un graphe reliant les concepts extraits. \"\"\"\n",
    "    graphe = nx.Graph()\n",
    "    for i, concept1 in enumerate(concepts):\n",
    "        for j, concept2 in enumerate(concepts):\n",
    "            if i != j:  # On évite de relier un concept à lui-même\n",
    "                graphe.add_edge(concept1, concept2)\n",
    "    return graphe\n",
    "\n",
    "\n",
    "# Traitement des documents dans chaque module\n",
    "for module in modules:\n",
    "    collection = db[module]  # Accéder à la collection du module\n",
    "    documents = list(collection.find({}))  # Récupérer tous les documents\n",
    "\n",
    "    for doc in documents:\n",
    "        texte = nettoyer_texte(doc[\"content\"])\n",
    "\n",
    "        # Extraction des concepts\n",
    "        mots_cles_tfidf = extraire_mots_cles_tfidf([doc])\n",
    "        entites_spacy = extraire_concepts(texte)\n",
    "\n",
    "        # Fusion des mots-clés et entités pour obtenir une meilleure liste de concepts\n",
    "        concepts_extraits = list(set(mots_cles_tfidf).union(set(entites_spacy)))\n",
    "\n",
    "        # Limiter la taille des concepts et relations pour éviter le dépassement de taille\n",
    "        concepts_extraits = concepts_extraits[:100]  # Limiter à 100 concepts maximum\n",
    "        graphe = creer_graphe_connaissance(concepts_extraits)\n",
    "\n",
    "        # Limiter les relations à 100\n",
    "        relations = [{\"concept1\": edge[0], \"relation\": \"lié à\", \"concept2\": edge[1]} for edge in graphe.edges][:100]\n",
    "\n",
    "        # Mise à jour du document dans MongoDB\n",
    "        collection.update_one(\n",
    "            {\"_id\": doc[\"_id\"]},\n",
    "            {\"$set\": {\"concepts\": concepts_extraits, \"relations\": relations}}\n",
    "        )\n",
    "\n",
    "    print(f\"Traitement terminé pour le module : {module}\")\n",
    "\n",
    "print(\"Analyse terminée, données mises à jour dans MongoDB.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
